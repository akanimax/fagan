# configuration for the discriminator architecture

architecture:

  -
    name: "full_attention"
    channels: (3, 64)
    kernel_dims: (4, 4)
    stride: (2, 2)
    padding: (1, 1)
    bias: True
    use_spectral_norm: True
    use_batch_norm: False
    squeeze_factor: 1
    transpose_conv: False
    activation: "lrelu(0.1)"

  -
    name: "ignore_attn_maps"

  -
    name: "conv"
    channels: (64, 128)
    kernel_dims: (4, 4)
    stride: (2, 2)
    padding: (1, 1)
    bias: True
    batch_norm: False
    spectral_norm: True
    activation: "lrelu(0.1)"

  -
    name: "full_attention"
    channels: (128, 256)
    kernel_dims: (4, 4)
    stride: (2, 2)
    padding: (1, 1)
    bias: True
    use_spectral_norm: True
    use_batch_norm: True
    squeeze_factor: 8
    transpose_conv: False
    activation: "lrelu(0.3)"

  -
    name: "ignore_attn_maps"

  -
    name: "self_attention"
    channels: 256
    bias: True
    squeeze_factor: 8

  -
    name: "ignore_attn_maps"

  -
    name: "conv"
    channels: (256, 512)
    kernel_dims: (4, 4)
    stride: (2, 2)
    padding: (1, 1)
    bias: True
    batch_norm: False
    spectral_norm: True
    activation: "lrelu(0.1)"

  -
    name: "self_attention"
    channels: 512
    bias: True
    squeeze_factor: 8

  -
    name: "ignore_attn_maps"

  -
    name: "conv"
    channels: (512, 1)
    kernel_dims: (4, 4)
    stride: (1, 1)
    padding: (0, 0)
    bias: True
    batch_norm: False
    spectral_norm: False
    activation: "lrelu(1.0)"
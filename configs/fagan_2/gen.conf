# configuration for the Generator architecture

architecture:
  -
    name: "full_attention"
    channels: (128, 512)
    kernel_dims: (4, 4)
    stride: (1, 1)
    padding: (0, 0)
    bias: True
    use_spectral_norm: True
    use_batch_norm: True
    squeeze_factor: 8
    transpose_conv: True
    activation: "lrelu(0.3)"

  -
    name: "ignore_attn_maps"

  -
    name: "full_attention"
    channels: (512, 256)
    kernel_dims: (4, 4)
    stride: (2, 2)
    padding: (1, 1)
    bias: True
    use_spectral_norm: True
    use_batch_norm: True
    squeeze_factor: 8
    transpose_conv: True
    activation: "lrelu(0.3)"

  -
    name: "ignore_attn_maps"

  -
    name: "conv_transpose"
    channels: (256, 128)
    kernel_dims: (4, 4)
    stride: (2, 2)
    padding: (1, 1)
    bias: True
    batch_norm: True
    spectral_norm: True
    activation: "relu"

  -
    name: "self_attention"
    channels: 128
    bias: True
    squeeze_factor: 8

  -
    name: "ignore_attn_maps"

  -
    name: "conv_transpose"
    channels: (128, 64)
    kernel_dims: (4, 4)
    stride: (2, 2)
    padding: (1, 1)
    bias: True
    batch_norm: True
    spectral_norm: True
    activation: "relu"

  -
    name: "self_attention"
    channels: 64
    bias: True
    squeeze_factor: 8

  -
    name: "ignore_attn_maps"

  -
    name: "conv_transpose"
    channels: (64, 3)
    kernel_dims: (4, 4)
    stride: (2, 2)
    padding: (1, 1)
    bias: True
    batch_norm: False
    spectral_norm: False
    activation: "tanh"
